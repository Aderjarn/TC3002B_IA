# -*- coding: utf-8 -*-
"""ModelTraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dk56GduI2SCl_8rIZovw38L3qjsIk0mK
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/8° Semestre/IA M2 Benji"
!ls

"""# Data Augmentation

usamos ImageDataGenerator para generar imagenes nuevas en el ram mientras entrenamos para no desperdiciar espacio.
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_dir = 'Dataset2_1'
train_dir = os.path.join(base_dir,'Train')
test_dir = os.path.join(base_dir, 'Test')

class_names = sorted(os.listdir(train_dir))
class_labels = {class_name: i for i, class_name in enumerate(class_names)}

train_datagen = ImageDataGenerator(
							rescale = 1./255,
							rotation_range = 60,
						#	width_shift_range = 0.2,
						#	height_shift_range = 0.2,
						#	shear_range = 0.3,
							zoom_range = 0.3,
							horizontal_flip = True,
							preprocessing_function=tf.image.rgb_to_grayscale,
							validation_split=0.2
							)

"""para ello tenemos que asignar una fuente base de imágenes a modificar, en este caso tomamos las de el directorio de train, en este caso solo estamos tomando una imagen y estamos mostrando como se generarían 5 imágenes diferentes:"""

import matplotlib.pyplot as plt

train_generator = train_datagen.flow_from_directory(
		 train_dir,
		 target_size = (64, 64),
		 batch_size = 1,
		 #class_mode ='binary',
		 class_mode ='categorical',
		 subset='training',
		 classes=class_names,  # Pass class labels
		 shuffle=True,
		 seed=42
		 )

validation_generator = train_datagen.flow_from_directory(
		 'Dataset2_1/Train',
		 target_size=(64, 64),
		 batch_size=8,
		 class_mode='categorical',
		 subset='validation',
		 classes=class_names,  # Pass class labels
		 shuffle=True,
		 seed=42
		 )

plt.figure()
#subplot(r,c) provide the no. of rows and columns
f, axarr = plt.subplots(1, 5, figsize=(30, 8))

for i in range(5) :
  axarr[i].imshow(train_generator[0][0][0], cmap='gray')

"""En este caso estamos tomando 8 imágenes y estomos modificando todas 1 vez cada una y las mostramos a manera de ejemplo."""

train_generator = train_datagen.flow_from_directory(
		 train_dir,
		 target_size = (64, 64),
		 batch_size = 8,
		 #class_mode ='binary',
		 class_mode ='categorical',
		 subset='training',
		 classes=class_names,  # Pass class labels
		 shuffle=True,
		 seed=42
		 )

images, labels = train_generator[0]

print(images.shape)
print(labels)

plt.figure()
#subplot(r,c) provide the no. of rows and columns
f, axarr = plt.subplots(1, images.shape[0], figsize=(30, 4))

for i in range(images.shape[0]) :
  axarr[i].imshow(images[i])

"""También podemos configurar la generación de imágenes para que cada una se salve en el disco duro, de esta forma podemos ver posteriormente con qué imágenes entrenó el modelo.

path = "/content/drive/MyDrive/8° Semestre/IA M2 Benji/Dataset2_1"

train_generator = train_datagen.flow_from_directory(
							train_dir,
							target_size = (150, 150),
							batch_size = 8,
							#class_mode ='binary',
							class_mode ='categorical',
							save_to_dir= path + '/Augmented',
              save_prefix='aug',
              save_format='png'
							)

Esta es una red neuronal convolutiva:
"""

from tensorflow.keras import optimizers, callbacks
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras.optimizers.schedules import LearningRateSchedule

"""initial_learning_rate = 1e-4
decay_steps = len(os.listdir(train_dir)) // 8
decay_rate = 0.75"""

model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation="relu", input_shape=(64, 64, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation="relu"),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation="relu"),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(28, activation='softmax')
])

model.summary()
model.compile(loss="categorical_crossentropy",
              optimizer="adam",
              metrics=['accuracy'])

"""class LRTuningCallback(callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.lr = []
        self.loss = []

    def on_epoch_end(self, epoch, logs=None):
        current_lr = self.model.optimizer.lr
        current_loss = logs.get('loss')
        self.lr.append(current_lr)
        self.loss.append(current_loss)"""

def early_stopping(monitor='val_loss', min_delta=0, patience=5):
    """
    Implements early stopping based on the given monitor metric.

    Parameters:
    - monitor (str): The metric to monitor. Can be 'val_loss' or 'val_accuracy'.
    - min_delta (float): Minimum change in the monitored metric to qualify as improvement.
    - patience (int): Number of epochs with no improvement after which training will be stopped.

    Returns:
    - keras.callbacks.EarlyStopping: Early stopping callback.
    """
    return callbacks.EarlyStopping(monitor=monitor, min_delta=min_delta, patience=patience, restore_best_weights=True)

#early_stop = early_stopping(monitor='loss', patience=3)  # Monitor training loss
early_stop = early_stopping(monitor='val_loss', patience=3)  # Monitor validation loss
history = model.fit(train_generator, epochs=15, validation_data=validation_generator, callbacks=[early_stop])
"""history = model.fit(
						train_generator,
						epochs = 15,
						callbacks=[early_stop])"""

acc = history.history['accuracy']
loss = history.history['loss']

epochs = range(1, len(acc)+1)

plt.plot(epochs,acc,'bo',label='train accuracy')
plt.title('train acc')
plt.legend()

plt.figure()

plt.plot(epochs,loss, 'bo', label ='training loss')
plt.title('train loss')
plt.legend()

test_datagen = ImageDataGenerator(
		 rescale=1./255,
		 preprocessing_function=tf.image.rgb_to_grayscale,
		 )  # Normalize pixel values
test_generator = test_datagen.flow_from_directory(
		 test_dir,
		 target_size=(64, 64),
		 batch_size=20,
		 class_mode='categorical',
		 classes=class_names,
		 shuffle=False
		 )

# Calculate the number of steps needed to iterate over the entire test dataset
num_test_samples = len(test_generator.filenames)
num_test_steps = num_test_samples // test_generator.batch_size

# Evaluate the model on the test data generator
test_loss, test_acc = model.evaluate(test_generator, steps=num_test_steps)
print('\nTest accuracy:', test_acc)

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

# Get true labels and filenames
true_labels = test_generator.classes
filenames = test_generator.filenames

# Generate predictions for test data
predictions = model.predict(test_generator)
predicted_labels = np.argmax(predictions, axis=1)

# Print predictions
print("Filename\t\t\tTrue Label\tPredicted Label")
print("------------------------------------------------------------")
for filename, true_label, predicted_label in zip(filenames, true_labels, predicted_labels):
    print("{:<30}\t{}\t\t{}".format(filename, true_label, predicted_label))

# Get the class labels from the test data generator
class_labels = list(test_generator.class_indices.keys())

# Generate predictions for the test data
predictions = model.predict(test_generator)

# Print out probabilities for each class for each image
for i in range(len(predictions)):
    print("Image:", test_generator.filenames[i])
    print("True Label:", class_labels[true_labels[i]])
    print("Predicted Probabilities:")
    for j in range(len(class_labels)):
        print("{}: {:.2f}%".format(class_labels[j], predictions[i][j] * 100))
    print("------------------------------------------")

test_imgs = test_generator[0][0]
test_labels = test_generator[0][1]


predictions = model.predict(test_imgs)
classes_x = np.argmax(predictions,axis=1)
classes_x
test_labels_int = np.argmax(test_labels, axis=1)

print('Model         ', 'test loss            ', ' test accuracy ')
print('Original      ', test_loss, '   ', test_acc)
#print('More Layers   ', test_loss_cnn, '   ', test_acc_cnn)
#print('Transfer VGG  ', test_loss_vgg, '  ', test_acc_vgg)


from tensorflow.math import confusion_matrix

mat = confusion_matrix(classes_x, test_labels_int)
print('         ', 'label neg ', ' label pos')
print('pred neg    ', np.array(mat[0][0]), "        ", np.array(mat[0][1]))
print('pred pos    ', np.array(mat[1][0]), "         ", np.array(mat[1][1]))

"""1 model.save("hermno.keras")

model = models.load_model(hermno.keras)
"""